{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elmo 3.4114394716164127e+18\n",
      "xlnet 3.856582065028608e+21\n",
      "gpt-infer 129752659968\n",
      "gpt 3.968752230785352e+19\n",
      "bert_small 1.4187399146496e+18\n",
      "bert_base 6.428335104e+19\n",
      "bert_large 1.91720905883648e+20\n",
      "electra_small 1.293743798272e+18\n",
      "electra_base 6.424541834277683e+19\n",
      "electra_400k 7.117606040895488e+20\n",
      "electra_1.75M 3.113952642891776e+21\n",
      "roberta 3.187221435776e+21\n",
      "albert 3.113923955392512e+22\n",
      "t5_11b 4.581900124212429e+22\n",
      "opt-30B 31115489435648\n",
      "opt-13B 16029859307520\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Computes the flops needed for training/running transformer networks.\"\"\"\n",
    "\n",
    "import collections\n",
    "\n",
    "# We checked this code with TensorFlow\"s FLOPs counting, although we had to\n",
    "# correct for this issue: https://github.com/tensorflow/tensorflow/issues/22071\n",
    "# Assumptions going into the FLOPs counting\n",
    "#   - An \"operation\" is a mathematical operation, not a machine instruction. So\n",
    "#     an \"exp\" takes one opp like and add, even though in practice an exp\n",
    "#     might be slower. This is not too bad an assumption because\n",
    "#     matrix-multiplies dominate the compute for most models, so minor details\n",
    "#     about activation functions don\"t matter too much. Similarly, we count\n",
    "#     matrix-multiplies as 2*m*n flops instead of m*n, as one might if\n",
    "#     if considering fused multiply-add ops.\n",
    "#   - Backward pass takes the same number of FLOPs as forward pass. No exactly\n",
    "#     right (e.g., for softmax cross entropy loss the backward pass is faster).\n",
    "#     Importantly, it really is the same for matrix-multiplies, which is most of\n",
    "#     the compute anyway.\n",
    "#   - We assume \"dense\" embedding lookups (i.e., multiplication by a one-hot\n",
    "#     vector). On some hardware accelerators, these dense operations are\n",
    "#     actually faster than sparse lookups.\n",
    "# Please open a github issue if you spot a problem with this code!\n",
    "\n",
    "# I am not sure if the below constants are 100% right, but they are only applied\n",
    "# to O(hidden_size) activations, which is generally a lot less compute than the\n",
    "# matrix-multiplies, which are O(hidden_size^2), so they don't affect the total\n",
    "# number of FLOPs much.\n",
    "\n",
    "# random number, >=, multiply activations by dropout mask, multiply activations\n",
    "# by correction (1 / (1 - dropout_rate))\n",
    "DROPOUT_FLOPS = 4\n",
    "\n",
    "# compute mean activation (sum), computate variance of activation\n",
    "# (square and sum), bias (add), scale (multiply)\n",
    "LAYER_NORM_FLOPS = 5\n",
    "\n",
    "# GELU: 0.5 * x * (1 + tanh(sqrt(2 / np.pi) * (x + 0.044715 * pow(x, 3))))\n",
    "ACTIVATION_FLOPS = 8\n",
    "\n",
    "# max/substract (for stability), exp, sum, divide\n",
    "SOFTMAX_FLOPS = 5\n",
    "\n",
    "\n",
    "class TransformerHparams(object):\n",
    "  \"\"\"Computes the train/inference FLOPs for transformers.\"\"\"\n",
    "\n",
    "  def __init__(self, h, l, s=512, v=30522, e=None, i=None, heads=None,\n",
    "      head_size=None, output_frac=0.15625, sparse_embed_lookup=False,\n",
    "      decoder=False):\n",
    "    self.h = h  # hidden size\n",
    "    self.l = l  # number of layers\n",
    "    self.s = s  # sequence length\n",
    "    self.v = v  # vocab size\n",
    "    self.e = h if e is None else e  # embedding size\n",
    "    self.i = h * 4 if i is None else i  # intermediate size\n",
    "    self.kqv = h if head_size is None else head_size * heads  # attn proj sizes\n",
    "    self.heads = max(h // 64, 1) if heads is None else heads  # attention heads\n",
    "    self.output_frac = output_frac  # percent of tokens using an output softmax\n",
    "    self.sparse_embed_lookup = sparse_embed_lookup  # sparse embedding lookups\n",
    "    self.decoder = decoder  # decoder has extra attn to encoder states\n",
    "\n",
    "  def get_block_flops(self):\n",
    "    \"\"\"Get the forward-pass FLOPs for a single transformer block.\"\"\"\n",
    "    attn_mul = 2 if self.decoder else 1\n",
    "    block_flops = dict(\n",
    "        kqv=3 * 2 * self.h * self.kqv * attn_mul,\n",
    "        kqv_bias=3 * self.kqv * attn_mul,\n",
    "        attention_scores=2 * self.kqv * self.s * attn_mul,\n",
    "        attn_softmax=SOFTMAX_FLOPS * self.s * self.heads * attn_mul,\n",
    "        attention_dropout=DROPOUT_FLOPS * self.s * self.heads * attn_mul,\n",
    "        attention_scale=self.s * self.heads * attn_mul,\n",
    "        attention_weighted_avg_values=2 * self.h * self.s * attn_mul,\n",
    "        attn_output=2 * self.h * self.h * attn_mul,\n",
    "        attn_output_bias=self.h * attn_mul,\n",
    "        attn_output_dropout=DROPOUT_FLOPS * self.h * attn_mul,\n",
    "        attn_output_residual=self.h * attn_mul,\n",
    "        attn_output_layer_norm=LAYER_NORM_FLOPS * attn_mul,\n",
    "        intermediate=2 * self.h * self.i,\n",
    "        intermediate_act=ACTIVATION_FLOPS * self.i,\n",
    "        intermediate_bias=self.i,\n",
    "        output=2 * self.h * self.i,\n",
    "        output_bias=self.h,\n",
    "        output_dropout=DROPOUT_FLOPS * self.h,\n",
    "        output_residual=self.h,\n",
    "        output_layer_norm=LAYER_NORM_FLOPS * self.h,\n",
    "    )\n",
    "    return sum(block_flops.values()) * self.s\n",
    "\n",
    "  def get_embedding_flops(self, output=False):\n",
    "    \"\"\"Get the forward-pass FLOPs the transformer inputs or output softmax.\"\"\"\n",
    "    embedding_flops = {}\n",
    "    if output or (not self.sparse_embed_lookup):\n",
    "      embedding_flops[\"main_multiply\"] = 2 * self.e * self.v\n",
    "    # input embedding post-processing\n",
    "    if not output:\n",
    "      embedding_flops.update(dict(\n",
    "          tok_type_and_position=2 * self.e * (self.s + 2),\n",
    "          add_tok_type_and_position=2 * self.e,\n",
    "          emb_layer_norm=LAYER_NORM_FLOPS * self.e,\n",
    "          emb_dropout=DROPOUT_FLOPS * self.e\n",
    "      ))\n",
    "    # projection layer if e != h\n",
    "    if self.e != self.h or output:\n",
    "      embedding_flops.update(dict(\n",
    "          hidden_kernel=2 * self.h * self.e,\n",
    "          hidden_bias=self.e if output else self.h\n",
    "      ))\n",
    "      # extra hidden layer and output softmax\n",
    "      if output:\n",
    "        embedding_flops.update(dict(\n",
    "            hidden_activation=ACTIVATION_FLOPS * self.e,\n",
    "            hidden_layernorm=LAYER_NORM_FLOPS * self.e,\n",
    "            output_softmax=SOFTMAX_FLOPS * self.v,\n",
    "            output_target_word=2 * self.v\n",
    "        ))\n",
    "        return self.output_frac * sum(embedding_flops.values()) * self.s\n",
    "    return sum(embedding_flops.values()) * self.s\n",
    "\n",
    "  def get_binary_classification_flops(self):\n",
    "    classification_flops = dict(\n",
    "        hidden=2 * self.h * self.h,\n",
    "        hidden_bias=self.h,\n",
    "        hidden_act=ACTIVATION_FLOPS * self.h,\n",
    "        logits=2 * self.h\n",
    "    )\n",
    "    return sum(classification_flops.values()) * self.s\n",
    "\n",
    "  def get_train_flops(self, batch_size, train_steps, discriminator=False):\n",
    "    \"\"\"Get the FLOPs for pre-training the transformer.\"\"\"\n",
    "    # 2* for forward/backward pass\n",
    "    return 2 * batch_size * train_steps * (\n",
    "        (self.l * self.get_block_flops()) +\n",
    "        self.get_embedding_flops(output=False) +\n",
    "        (self.get_binary_classification_flops() if discriminator else\n",
    "         self.get_embedding_flops(output=True))\n",
    "    )\n",
    "\n",
    "  def get_infer_flops(self):\n",
    "    \"\"\"Get the FLOPs for running inference with the transformer on a\n",
    "    classification task.\"\"\"\n",
    "    return ((self.l * self.get_block_flops()) +\n",
    "            self.get_embedding_flops(output=False) +\n",
    "            self.get_binary_classification_flops())\n",
    "\n",
    "\n",
    "def get_electra_train_flops(\n",
    "    h_d, l_d, h_g, l_g, batch_size, train_steps, tied_embeddings,\n",
    "    e=None, s=512, output_frac=0.15625):\n",
    "  \"\"\"Get the FLOPs needed for  pre-training ELECTRA.\"\"\"\n",
    "  if e is None:\n",
    "    e = h_d\n",
    "  disc = TransformerHparams(\n",
    "      h_d, l_d, s=s, e=e,\n",
    "      output_frac=output_frac).get_train_flops(batch_size, train_steps, True)\n",
    "  gen = TransformerHparams(\n",
    "      h_g, l_g, s=s, e=e if tied_embeddings else None,\n",
    "      output_frac=output_frac).get_train_flops(batch_size, train_steps)\n",
    "  return disc + gen\n",
    "\n",
    "\n",
    "MODEL_FLOPS = collections.OrderedDict([\n",
    "    # These runtimes were computed with tensorflow FLOPs counting instead of the\n",
    "    # script, as the neural architectures are quite different.\n",
    "    # 768648884 words in LM1b benchmark, 10 epochs with batch size 20,\n",
    "    # seq length 128, 568093262680 FLOPs per example.\n",
    "    (\"elmo\", 2 * 10 * 768648884 * 568093262680 / (20.0 * 128)),\n",
    "    # 15064773691518 is FLOPs for forward pass on 32 examples.\n",
    "    # Therefore 2 * steps * batch_size * 15064773691518 / 32 is XLNet compute\n",
    "    (\"xlnet\", 2 * 500000 * 8192 * 15064773691518 / 32.0),\n",
    "    # (\"gpt-energon\", TransformerHparams(768, 12, s=1, v=10000, output_frac=1.0).get_train_flops(\n",
    "    #     128, 960800)),\n",
    "    # Runtimes computed with the script\n",
    "     (\"gpt-infer\", TransformerHparams(768, 12, v=40000, output_frac=1.0).get_infer_flops()),\n",
    "    (\"gpt\", TransformerHparams(768, 12, v=40000, output_frac=1.0).get_train_flops(\n",
    "        128, 960800)),\n",
    "    (\"bert_small\", TransformerHparams(256, 12, e=128, s=128).get_train_flops(128, 1.45e6)),\n",
    "    (\"bert_base\", TransformerHparams(768, 12).get_train_flops(256, 1e6)),\n",
    "    (\"bert_large\", TransformerHparams(1024, 24).get_train_flops(256, 1e6)),\n",
    "    (\"electra_small\", get_electra_train_flops(256, 12, 64, 12, 128, 1e6, True, s=128, e=128)),\n",
    "    (\"electra_base\", get_electra_train_flops(768, 12, 256, 12, 256, 766000, True)),\n",
    "    (\"electra_400k\", get_electra_train_flops(1024, 24, 256, 24, 2048, 400000, True)),\n",
    "    (\"electra_1.75M\", get_electra_train_flops(1024, 24, 256, 24, 2048, 1750000, True)),\n",
    "\n",
    "    # RoBERTa, ALBERT, and T5 have  minor architectural differences from\n",
    "    # BERT/ELECTRA, but I believe they don't significantly effect the runtime,\n",
    "    # so we use this script for those models as well.\n",
    "    (\"roberta\", TransformerHparams(1024, 24, v=50265).get_train_flops(8000, 500000)),\n",
    "    (\"albert\", TransformerHparams(4096, 12, v=30000, e=128).get_train_flops(\n",
    "        4096, 1.5e6)),\n",
    "    (\"t5_11b\", TransformerHparams(\n",
    "        1024,  # hidden size\n",
    "        24,  # layers\n",
    "        v=32000,  # vocab size\n",
    "        i=65536,  # ff intermediate hidden size\n",
    "        heads=128, head_size=128,  # heads/head size\n",
    "        output_frac=0.0  # encoder has no output softmax\n",
    "    ).get_train_flops(2048, 1e6) +  # 1M steps with batch size 2048\n",
    "     TransformerHparams(\n",
    "         1024,\n",
    "         24,\n",
    "         v=32000,\n",
    "         i=65536,\n",
    "         heads=128, head_size=128,\n",
    "         output_frac=1.0,  # decoder has output softmax for all positions\n",
    "         decoder=True\n",
    "     ).get_train_flops(2048, 1e6))\n",
    "])\n",
    "\n",
    "\n",
    "def main():\n",
    "  for k, v in MODEL_FLOPS.items():\n",
    "    print(k, v)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()\n",
    "  res=collections.OrderedDict([\n",
    "    (\"opt-30B\", TransformerHparams(7168, 48, v=50272, output_frac=1.0).get_infer_flops()),\n",
    "    (\"opt-13B\", TransformerHparams(5120, 48, v=50272, output_frac=1.0).get_infer_flops())\n",
    "  ])\n",
    "  for k, v in res.items():\n",
    "    print(k, v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162473.802753584\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocab = 50272 # vocab可以调整\n",
    "d_model=512 \n",
    "d_ff=2048 \n",
    "h=40 \n",
    "d_k=64\n",
    "d_out = 1\n",
    "tgt_vocab = 10000\n",
    "encoder = 4 * vocab *d_model +vocab + 6 * (16* vocab *d_model + h * (6*d_model*vocab*d_k - 4*vocab*d_k + 4*d_k*vocab*vocab + 3*vocab*vocab - vocab) +2 * vocab * d_model *d_model - vocab * d_model + 2 * vocab + 4 * vocab * d_model *d_ff)\n",
    "decoder = d_out * ( 4 * d_model +1) + 6 * (16 * d_out *d_model + 2 * d_out * d_model * d_model + 4*d_model*d_out*d_ff + 4 * d_model * vocab + vocab +2 * d_model * d_model * d_out + h * (8 * d_model * d_out * d_k - 6 * d_k + 4 * d_k * d_out * d_out + 3 * d_out * d_out - 2 * d_out +4 * d_model * vocab * d_k - 2 * d_k * vocab + 4 * d_k * d_out * vocab + 3 * vocab * d_out))\n",
    "generator = 2 * tgt_vocab * d_out - d_out + 2 * d_model * tgt_vocab * d_out\n",
    " \n",
    "result = (encoder + decoder + generator) / 1e9\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "267",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
